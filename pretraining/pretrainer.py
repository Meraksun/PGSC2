import torch
import torch.nn as nn
import torch.optim as optim
from typing import Optional, Dict, List
from torch.utils.data import DataLoader
from tqdm import tqdm  # ç”¨äºè®­ç»ƒè¿›åº¦å¯è§†åŒ–ï¼ˆå¯é€‰ï¼Œæå‡ä½“éªŒï¼‰

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆéœ€ç¡®ä¿å„æ¨¡å—è·¯å¾„æ­£ç¡®ï¼‰
from gnn_layers import DyMPNLayer, GraphMultiHeadAttention
from physics_loss import PhysicsInformedLoss


class GTransformerPretrain(nn.Module):
    """
    åŸºäºè®ºæ–‡çš„Graph Transformeré¢„è®­ç»ƒæ¨¡å‹ï¼ˆPPGTç®€åŒ–ç‰ˆï¼‰
    æ ¸å¿ƒç»“æ„ï¼šå †å "DyMPNå±€éƒ¨ç‰¹å¾æå– + GraphMultiHeadAttentionå…¨å±€ä¾èµ–æ•æ‰"å±‚
    é€‚é…20-50èŠ‚ç‚¹è¾å°„å‹é…ç”µç½‘ï¼Œèšç„¦æ©ç ç”µå‹é¢„æµ‹ä»»åŠ¡ï¼ˆğŸ”¶1-20ã€ğŸ”¶1-22ã€ğŸ”¶1-75ï¼‰
    """

    def __init__(
            self,
            d_in: int = 4,
            d_model: int = 64,
            n_heads: int = 4,
            n_layers: int = 2
    ):
        """
        åˆå§‹åŒ–GTransformeré¢„è®­ç»ƒæ¨¡å‹

        Args:
            d_in: è¾“å…¥èŠ‚ç‚¹ç‰¹å¾ç»´åº¦ï¼ˆé»˜è®¤4ï¼Œå¯¹åº”P_loadã€Q_loadã€Vã€Î¸æ ‡å¹ºå€¼ï¼‰
            d_model: åµŒå…¥/ä¸­é—´ç‰¹å¾ç»´åº¦ï¼ˆé»˜è®¤64ï¼Œä¸DyMPNã€æ³¨æ„åŠ›å±‚ä¸€è‡´ï¼‰
            n_heads: æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤4ï¼Œéœ€æ»¡è¶³d_model % n_heads == 0ï¼Œé€‚é…å°èŠ‚ç‚¹ï¼‰
            n_layers: GTransformerå †å å±‚æ•°ï¼ˆé»˜è®¤2ï¼Œå°èŠ‚ç‚¹è§„æ¨¡æ— éœ€æ·±å±‚ç»“æ„ï¼‰
        """
        super().__init__()
        self.d_model = d_model
        self.n_layers = n_layers

        # 1. è¾“å…¥åµŒå…¥å±‚ï¼šå°†åŸå§‹4ç»´ç‰¹å¾æ˜ å°„åˆ°d_modelç»´ï¼ˆä¸DyMPNè¾“å…¥åŒ¹é…ï¼‰
        self.input_embed = nn.Linear(d_in, d_model)

        # 2. å †å GTransformerå±‚ï¼šæ¯å±‚=DyMPN + GraphMultiHeadAttention + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        self.gtransformer_layers = nn.ModuleList()
        for _ in range(n_layers):
            dympn = DyMPNLayer(d_in=d_model, d_model=d_model)  # DyMPNè¾“å…¥ä¸ºd_modelï¼ˆåµŒå…¥åç‰¹å¾ï¼‰
            gatt = GraphMultiHeadAttention(d_model=d_model, n_heads=n_heads)
            norm = nn.LayerNorm(d_model)  # å±‚å½’ä¸€åŒ–ï¼ˆè®ºæ–‡ä¸­ç”¨äºç¨³å®šè®­ç»ƒï¼ŒğŸ”¶1-72ï¼‰
            self.gtransformer_layers.append(nn.ModuleDict({
                "dympn": dympn,
                "gatt": gatt,
                "norm": norm
            }))

        # 3. èŠ‚ç‚¹ç‰¹å¾é¢„æµ‹å¤´ï¼šè¾“å‡ºå®Œæ•´èŠ‚ç‚¹ç‰¹å¾ï¼ˆP_loadã€Q_loadã€Vã€Î¸ï¼Œ4ç»´ï¼‰
        # è®ºæ–‡ä¸­é¢„è®­ç»ƒç›®æ ‡ä¸º"è¡¥å…¨æ©ç ç‰¹å¾"ï¼Œæ­¤å¤„ç›´æ¥é¢„æµ‹æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾ï¼ˆğŸ”¶1-78ï¼‰
        self.node_pred_head = nn.Linear(d_model, 4)

    def forward(
            self,
            node_feat: torch.Tensor,
            adj: torch.Tensor,
            node_count: torch.Tensor
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šä»å¸¦æ©ç çš„èŠ‚ç‚¹ç‰¹å¾é¢„æµ‹å®Œæ•´èŠ‚ç‚¹ç‰¹å¾

        Args:
            node_feat: å¸¦æ©ç çš„è¾“å…¥èŠ‚ç‚¹ç‰¹å¾ï¼ˆB, N, d_inï¼‰ï¼ŒB=Batchï¼ŒN=æœ€å¤§èŠ‚ç‚¹æ•°
            adj: æ‹“æ‰‘é‚»æ¥çŸ©é˜µï¼ˆB, N, Nï¼‰
            node_count: æ¯ä¸ªåœºæ™¯çš„çœŸå®èŠ‚ç‚¹æ•°ï¼ˆB, ï¼‰

        Returns:
            pred_node: é¢„æµ‹çš„å®Œæ•´èŠ‚ç‚¹ç‰¹å¾ï¼ˆB, N, 4ï¼‰
        """
        # æ­¥éª¤1ï¼šè¾“å…¥ç‰¹å¾åµŒå…¥ï¼ˆd_inâ†’d_modelï¼‰
        # å½¢çŠ¶å˜åŒ–ï¼š(B, N, d_in) â†’ (B, N, d_model)
        x = self.input_embed(node_feat)

        # æ­¥éª¤2ï¼šç»è¿‡n_layerså±‚GTransformer
        for layer in self.gtransformer_layers:
            # 2.1 æ®‹å·®è¿æ¥å¤‡ä»½ï¼ˆè®ºæ–‡ä¸­ç”¨äºç¼“è§£æ¢¯åº¦æ¶ˆå¤±ï¼ŒğŸ”¶1-72ï¼‰
            residual = x

            # 2.2 DyMPNï¼šæå–å±€éƒ¨æ‹“æ‰‘ç‰¹å¾
            local_feat = layer["dympn"](node_feat=x, adj=adj, node_count=node_count)

            # 2.3 GraphMultiHeadAttentionï¼šæ•æ‰å…¨å±€æ‹“æ‰‘ä¾èµ–
            # æ³¨æ„åŠ›æ©ç ï¼šä»node_featçš„æ©ç æ¨å¯¼ï¼ˆé0å³éæ©ç ï¼Œ0ä¸ºæ©ç ï¼‰
            # æ©ç é€»è¾‘ï¼šèŠ‚ç‚¹ç‰¹å¾ä¸­ç”µå‹åˆ—ï¼ˆ2ã€3åˆ—ï¼‰ä¸º0 â†’ è¯¥èŠ‚ç‚¹éœ€å±è”½æ³¨æ„åŠ›
            mask = (node_feat[:, :, 2:4] == 0).any(dim=-1, keepdim=True)  # (B, N, 1)
            global_feat = layer["gatt"](h=local_feat, mask=mask, node_count=node_count)

            # 2.4 æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
            x = layer["norm"](residual + global_feat)

        # æ­¥éª¤3ï¼šé¢„æµ‹å®Œæ•´èŠ‚ç‚¹ç‰¹å¾ï¼ˆd_modelâ†’4ï¼‰
        pred_node = self.node_pred_head(x)

        # æ­¥éª¤4ï¼šå±è”½å¡«å……èŠ‚ç‚¹çš„é¢„æµ‹ç»“æœï¼ˆå¡«å……èŠ‚ç‚¹ç‰¹å¾ç½®0ï¼Œé¿å…å¹²æ‰°æŸå¤±è®¡ç®—ï¼‰
        for b in range(pred_node.shape[0]):
            real_node = node_count[b].item()
            pred_node[b, real_node:, :] = 0.0

        return pred_node


def pretrain_loop(
        model: GTransformerPretrain,
        data_loader: DataLoader,
        loss_fn: PhysicsInformedLoss,
        optimizer: optim.Optimizer,
        epochs: int = 50,
        device: Optional[torch.device] = None,
        save_path: str = "./pretrained_weights.pth"
) -> None:
    """
    GTransformerè‡ªç›‘ç£é¢„è®­ç»ƒå¾ªç¯ï¼ˆåŸºäºæ©ç ç”µå‹é¢„æµ‹ä»»åŠ¡ï¼‰
    æ ¸å¿ƒé€»è¾‘ï¼šè®ºæ–‡"ç‰©ç†çŸ¥æƒ…è‡ªç›‘ç£é¢„è®­ç»ƒ"ç®€åŒ–ç‰ˆï¼Œä»…ä¿ç•™æ©ç ç‰¹å¾é¢„æµ‹ï¼ˆğŸ”¶1-23ã€ğŸ”¶1-75ï¼‰

    Args:
        model: GTransformerPretrainå®ä¾‹
        data_loader: æ•°æ®åŠ è½½å™¨ï¼ˆè¿”å›å¸¦æ©ç çš„èŠ‚ç‚¹ç‰¹å¾ã€çœŸå®æ ‡ç­¾ç­‰ï¼‰
        loss_fn: ç‰©ç†çŸ¥æƒ…æŸå¤±å‡½æ•°ï¼ˆPhysicsInformedLosså®ä¾‹ï¼‰
        optimizer: PyTorchä¼˜åŒ–å™¨ï¼ˆé»˜è®¤Adamï¼Œlr=1e-3ï¼‰
        epochs: é¢„è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤50ï¼Œå°æ•°æ®é›†æ— éœ€å¤šè½®ï¼‰
        device: è®­ç»ƒè®¾å¤‡ï¼ˆè‡ªåŠ¨æ£€æµ‹cpu/cudaï¼‰
        save_path: é¢„è®­ç»ƒæƒé‡ä¿å­˜è·¯å¾„
    """
    # 1. è®¾å¤‡è‡ªåŠ¨æ£€æµ‹ï¼ˆä¼˜å…ˆçº§ï¼šç”¨æˆ·æŒ‡å®š > è‡ªåŠ¨æ£€æµ‹ï¼‰
    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    loss_fn.to(device)
    print(f"=== å¼€å§‹é¢„è®­ç»ƒ | è®¾å¤‡: {device} | æ€»è½®æ¬¡: {epochs} | æƒé‡ä¿å­˜è·¯å¾„: {save_path} ===")

    # 2. é¢„è®­ç»ƒä¸»å¾ªç¯
    for epoch in range(1, epochs + 1):
        model.train()  # åˆ‡æ¢è®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨dropoutã€BatchNormç­‰ï¼‰
        total_epoch_loss = 0.0
        total_epoch_pred_loss = 0.0
        total_epoch_physics_loss = 0.0

        # éå†DataLoaderï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        with tqdm(data_loader, desc=f"Epoch {epoch}/{epochs}", unit="batch") as pbar:
            for batch_idx, batch in enumerate(pbar, 1):
                # 2.1 æ•°æ®ç§»è‡³è®¾å¤‡
                # Batché”®è¯´æ˜ï¼šæ¥è‡ªdata_loader.SceneDatasetçš„__getitem__
                node_feat = batch["node_matrix"].to(device)  # å¸¦æ©ç çš„èŠ‚ç‚¹ç‰¹å¾ (B, N, 4)
                adj = batch["adj_matrix"].to(device)  # é‚»æ¥çŸ©é˜µ (B, N, N)
                gt_node = batch["node_matrix"].to(device)  # çœŸå®èŠ‚ç‚¹ç‰¹å¾ï¼ˆæ ‡ç­¾ï¼‰(B, N, 4)
                gt_line = batch["line_matrix"].to(device)  # çœŸå®çº¿è·¯æ½®æµï¼ˆç®€åŒ–ç”¨ï¼ŒğŸ”¶1-88ï¼‰
                line_param = batch["line_matrix"].to(device)  # çº¿è·¯å‚æ•° (Bä¸ªå…ƒç´ ï¼Œæ¯ä¸ª(b_line,4))
                node_count = batch["node_matrix"].to(device)  # çœŸå®èŠ‚ç‚¹æ•° (B,)

                # 2.2 å‰å‘ä¼ æ’­ï¼šé¢„æµ‹å®Œæ•´èŠ‚ç‚¹ç‰¹å¾
                pred_node = model(node_feat=node_feat, adj=adj, node_count=node_count)

                # 2.3 è®¡ç®—æŸå¤±ï¼ˆè®ºæ–‡"é¢„æµ‹è¯¯å·®+ç‰©ç†çº¦æŸ"æŸå¤±ï¼ŒğŸ”¶1-82ï¼‰
                # ç®€åŒ–å¤„ç†ï¼šçº¿è·¯æ½®æµé¢„æµ‹æš‚ç”¨çœŸå®å€¼ï¼ˆgt_lineï¼‰ï¼Œä»…ä¼˜åŒ–èŠ‚ç‚¹ç‰¹å¾é¢„æµ‹
                total_loss, pred_loss, physics_loss = loss_fn(
                    pred_node=pred_node,
                    gt_node=gt_node,
                    pred_line=gt_line,  # ç®€åŒ–ï¼šç”¨çœŸå®çº¿è·¯æ½®æµæ›¿ä»£é¢„æµ‹ï¼ˆèšç„¦ç”µå‹é¢„æµ‹ï¼‰
                    gt_line=gt_line,
                    adj=adj,
                    line_param=line_param,
                    node_count=node_count
                )

                # 2.4 åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°
                optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
                total_loss.backward()  # è®¡ç®—æ¢¯åº¦
                optimizer.step()  # æ›´æ–°å‚æ•°

                # 2.5 ç´¯è®¡æŸå¤±ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                batch_size = node_feat.shape[0]
                total_epoch_loss += total_loss.item() * batch_size
                total_epoch_pred_loss += pred_loss.item() * batch_size
                total_epoch_physics_loss += physics_loss.item() * batch_size

                # 2.6 æ¯10ä¸ªBatchæ‰“å°æ—¥å¿—ï¼ˆğŸ”¶1-140ä¸­è®­ç»ƒç›‘æ§é€»è¾‘ï¼‰
                if batch_idx % 10 == 0:
                    avg_loss = total_epoch_loss / (batch_idx * batch_size)
                    avg_pred_loss = total_epoch_pred_loss / (batch_idx * batch_size)
                    avg_physics_loss = total_epoch_physics_loss / (batch_idx * batch_size)
                    pbar.set_postfix({
                        "æ€»æŸå¤±": f"{avg_loss:.6f}",
                        "é¢„æµ‹æŸå¤±": f"{avg_pred_loss:.6f}",
                        "ç‰©ç†æŸå¤±": f"{avg_physics_loss:.6f}"
                    })

        # 3. æ¯5è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹æƒé‡ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼Œä¾¿äºä¸­æ–­åæ¢å¤ï¼‰
        if epoch % 5 == 0:
            save_path_epoch = save_path.replace(".pth", f"_epoch{epoch}.pth")
            torch.save(model.state_dict(), save_path_epoch)
            print(f"âœ… Epoch {epoch} æƒé‡å·²ä¿å­˜è‡³: {save_path_epoch}")

    # 4. é¢„è®­ç»ƒç»“æŸï¼šä¿å­˜æœ€ç»ˆæƒé‡
    torch.save(model.state_dict(), save_path)
    print(f"\n=== é¢„è®­ç»ƒå®Œæˆ | æœ€ç»ˆæƒé‡ä¿å­˜è‡³: {save_path} ===")
    # è®¡ç®—å¹¶æ‰“å°æœ€ç»ˆå¹³å‡æŸå¤±
    avg_final_loss = total_epoch_loss / len(data_loader.dataset)
    avg_final_pred_loss = total_epoch_pred_loss / len(data_loader.dataset)
    avg_final_physics_loss = total_epoch_physics_loss / len(data_loader.dataset)
    print(
        f"ğŸ“Š æœ€ç»ˆå¹³å‡æŸå¤±ï¼šæ€»æŸå¤±={avg_final_loss:.6f}, é¢„æµ‹æŸå¤±={avg_final_pred_loss:.6f}, ç‰©ç†æŸå¤±={avg_final_physics_loss:.6f}")

# -------------------------- é¢„è®­ç»ƒå¯åŠ¨ç¤ºä¾‹ï¼ˆæ³¨é‡Šå½¢å¼ï¼Œå–æ¶ˆæ³¨é‡Šå¯è¿è¡Œï¼‰ --------------------------
# if __name__ == "__main__":
#     """
#     ç¤ºä¾‹ï¼šåˆå§‹åŒ–æ•°æ®é›†ã€æ¨¡å‹ã€æŸå¤±å‡½æ•°ï¼Œå¯åŠ¨é¢„è®­ç»ƒ
#     ä¾èµ–æ¨¡å—ï¼šdata_loaderï¼ˆSceneDatasetã€get_data_loaderï¼‰ã€physics_informed_lossï¼ˆPhysicsInformedLossï¼‰
#     """
#     # 1. å¯¼å…¥ä¾èµ–æ¨¡å—ï¼ˆéœ€ç¡®ä¿æ¨¡å—è·¯å¾„æ­£ç¡®ï¼‰
#     from data_loader import SceneDataset, get_data_loader
#
#     # 2. é…ç½®é¢„è®­ç»ƒå‚æ•°
#     PRETRAIN_CONFIG = {
#         "data_root": "./Dataset",       # æ•°æ®é›†è·¯å¾„ï¼ˆç”¨æˆ·å·²å‡†å¤‡ï¼‰
#         "mask_ratio": 0.3,              # ç”µå‹æ©ç æ¯”ä¾‹ï¼ˆè®ºæ–‡é»˜è®¤0.3ï¼ŒğŸ”¶1-78ï¼‰
#         "batch_size": 8,                # Batchå¤§å°ï¼ˆé€‚é…å°èŠ‚ç‚¹ï¼Œé¿å…GPUå†…å­˜ä¸è¶³ï¼‰
#         "epochs": 50,                   # é¢„è®­ç»ƒè½®æ¬¡
#         "lr": 1e-3,                     # å­¦ä¹ ç‡ï¼ˆAdamé»˜è®¤ï¼ŒğŸ”¶1-128ï¼‰
#         "save_path": "./pretrained_weights.pth",  # æƒé‡ä¿å­˜è·¯å¾„
#         "d_in": 4,                      # è¾“å…¥ç‰¹å¾ç»´åº¦
#         "d_model": 64,                  # åµŒå…¥ç»´åº¦
#         "n_heads": 4,                   # æ³¨æ„åŠ›å¤´æ•°
#         "n_layers": 2                   # GTransformerå±‚æ•°
#     }
#
#     # 3. åˆå§‹åŒ–æ•°æ®é›†ä¸DataLoader
#     print("=== åŠ è½½æ•°æ®é›† ===")
#     dataset = SceneDataset(
#         data_root=PRETRAIN_CONFIG["data_root"],
#         mask_ratio=PRETRAIN_CONFIG["mask_ratio"]
#     )
#     data_loader = get_data_loader(
#         dataset=dataset,
#         batch_size=PRETRAIN_CONFIG["batch_size"],
#         shuffle=True,
#         num_workers=2
#     )
#     print(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼šå…±{len(dataset)}ä¸ªåœºæ™¯ï¼ŒBatchå¤§å°={PRETRAIN_CONFIG['batch_size']}")
#
#     # 4. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨
#     print("\n=== åˆå§‹åŒ–æ¨¡å‹ä¸è®­ç»ƒç»„ä»¶ ===")
#     # 4.1 æ¨¡å‹
#     model = GTransformerPretrain(
#         d_in=PRETRAIN_CONFIG["d_in"],
#         d_model=PRETRAIN_CONFIG["d_model"],
#         n_heads=PRETRAIN_CONFIG["n_heads"],
#         n_layers=PRETRAIN_CONFIG["n_layers"]
#     )
#     # 4.2 ç‰©ç†çŸ¥æƒ…æŸå¤±å‡½æ•°ï¼ˆÎ»=0.5ï¼Œå¹³è¡¡é¢„æµ‹ä¸ç‰©ç†çº¦æŸï¼‰
#     loss_fn = PhysicsInformedLoss(lambda_=0.5)
#     # 4.3 ä¼˜åŒ–å™¨ï¼ˆAdamï¼Œè®ºæ–‡ä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨ç±»å‹ï¼ŒğŸ”¶1-128ï¼‰
#     optimizer = optim.Adam(model.parameters(), lr=PRETRAIN_CONFIG["lr"])
#     print(f"æ¨¡å‹å‚æ•°æ€»æ•°ï¼š{sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
#
#     # 5. å¯åŠ¨é¢„è®­ç»ƒ
#     print("\n=== å¯åŠ¨é¢„è®­ç»ƒ ===")
#     pretrain_loop(
#         model=model,
#         data_loader=data_loader,
#         loss_fn=loss_fn,
#         optimizer=optimizer,
#         epochs=PRETRAIN_CONFIG["epochs"],
#         save_path=PRETRAIN_CONFIG["save_path"]
#     )