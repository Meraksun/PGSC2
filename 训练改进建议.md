# 模型训练效果改进建议

## 🎯 关键问题识别

当前代码中发现的主要问题：
1. **线路潮流预测过于简化**（pretrainer.py 344-345行）：使用固定的线性组合，无法学习
2. **缺少学习率调度**：固定学习率可能导致训练不稳定
3. **缺少梯度裁剪**：可能导致训练爆炸
4. **缺少归一化层**：可能导致训练不稳定
5. **损失函数权重可能不平衡**

---

## 📋 改进方案（按优先级排序）

### 🔴 **优先级1：改进线路潮流预测（最重要）**

**问题**：当前使用硬编码的简化公式 `v_diff * 0.1 + p_diff`，无法学习真实的物理关系。

**解决方案**：在预训练模型中添加可学习的线路预测头

```python
# 在 GTransformerPretrain 类中添加
self.line_pred_head = nn.Sequential(
    nn.Linear(d_model * 2, d_model),  # 连接两个节点的特征
    nn.ReLU(),
    nn.Linear(d_model, 4)  # 输出 R, X, P, Q
)

# 在 forward 方法中使用可学习层替代简化公式
```

**预期效果**：大幅提升物理损失的准确性，提升模型性能。

---

### 🟠 **优先级2：添加学习率调度器和梯度裁剪**

**问题**：固定学习率1e-3可能导致训练后期震荡。

**解决方案**：

```python
# 1. 添加学习率调度器
from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau

# 余弦退火调度器
scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-5)

# 或基于损失的自适应调度器
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)

# 2. 添加梯度裁剪
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 在训练循环中：
for epoch in range(epochs):
    # ... 训练代码 ...
    optimizer.step()
    scheduler.step()  # 或 scheduler.step(val_loss) 用于 ReduceLROnPlateau
```

**预期效果**：训练更稳定，收敛更好。

---

### 🟡 **优先级3：添加归一化层和残差连接**

**问题**：缺少归一化层可能导致梯度消失/爆炸。

**解决方案**：在 DyMPN 和 GraphMultiHeadAttention 中添加 LayerNorm

```python
# 在 DyMPNLayer 中
self.norm1 = nn.LayerNorm(d_model)
self.norm2 = nn.LayerNorm(d_model)

# 在 forward 中
h = self.norm1(self.embed(node_feat))
h = self.norm2(h + aggregated)  # 残差连接
```

**预期效果**：训练更稳定，允许更深的网络。

---

### 🟢 **优先级4：改进损失函数权重**

**问题**：当前 λ=0.5 可能不是最优值。

**解决方案**：

1. **动态调整权重**：
```python
# 早期训练：更关注预测损失
# 后期训练：更关注物理损失
lambda_ = 0.3 if epoch < epochs // 2 else 0.7
```

2. **分别调整不同损失项的权重**：
```python
# 对节点损失和线路损失使用不同权重
node_loss_weight = 1.0
line_loss_weight = 0.5
physics_loss_weight = 0.5
```

---

### 🔵 **优先级5：数据增强策略**

**问题**：训练数据有限，容易过拟合。

**解决方案**：

1. **渐进式掩码**：
```python
# 训练初期：低掩码比例（0.2）
# 训练后期：高掩码比例（0.5）
mask_ratio = 0.2 + (epoch / epochs) * 0.3
```

2. **添加噪声增强**：
```python
# 对节点特征添加少量高斯噪声
noise = torch.randn_like(node_feat) * 0.01
node_feat = node_feat + noise
```

---

### 🟣 **优先级6：添加验证集监控**

**问题**：预训练阶段缺少验证集，无法监控过拟合。

**解决方案**：

```python
# 在预训练循环中添加验证步骤
if epoch % 5 == 0:  # 每5个epoch验证一次
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for batch in val_loader:
            # ... 验证代码 ...
    print(f"验证损失: {val_loss:.6f}")
```

---

### ⚪ **优先级7：模型架构优化**

1. **注意力层数调整**：尝试 n_layers=3 或 4
2. **嵌入维度调整**：尝试 d_model=128 或 256
3. **Dropout调整**：增加 dropout 防止过拟合（当前0.1）

---

## 🛠️ 快速实施建议

### 立即实施（30分钟内）：
1. ✅ 添加梯度裁剪（1行代码）
2. ✅ 添加学习率调度器（2行代码）
3. ✅ 添加验证集监控（10行代码）

### 中期实施（1-2小时）：
4. ✅ 改进线路潮流预测（改为可学习层）
5. ✅ 添加 LayerNorm 归一化层

### 长期优化（需要实验）：
6. ✅ 调整损失函数权重
7. ✅ 尝试数据增强
8. ✅ 调整模型架构超参数

---

## 📊 监控指标建议

添加以下监控指标：
- **学习率变化曲线**
- **梯度范数**（检测梯度爆炸）
- **损失函数各组件的变化**（pred_loss, physics_loss）
- **验证集性能**（如果有的话）
- **训练/验证曲线对比**（检测过拟合）

---

## 🎯 预期改进效果

按优先级实施后预期改进：

1. **改进线路预测** → 损失降低 20-30%
2. **学习率调度** → 收敛速度提升 15-20%
3. **归一化层** → 训练稳定性提升，允许更大学习率
4. **梯度裁剪** → 防止训练崩溃
5. **数据增强** → 泛化能力提升 10-15%

---

## 💡 实验建议

建议采用**消融实验**的方式，逐个添加改进，记录每个改进的效果：

1. Baseline（当前版本）
2. + 梯度裁剪 + 学习率调度
3. + 改进线路预测
4. + 添加归一化层
5. + 所有改进

这样可以明确每个改进的贡献。
